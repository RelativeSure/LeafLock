---
title: Performance Optimization
description: Tune LeafLock for low-latency encrypted collaboration.
sidebar:
  order: 3
---

import { Code } from '@astrojs/starlight/components';

## Runtime Overview
LeafLock is engineered to bring the HTTP stack online in under 5 seconds while background tasks prepare encrypted services. This guide focuses on tuning the crypto pipeline, WebSocket fan-out, and database access so administrators can meet latency and concurrency targets.

## Startup Pipeline
1. HTTP server binds and exposes `/api/v1/health/live` immediately. Full readiness is documented separately in `/reference/health-checks`.
2. Migrations execute while Redis and pgx pools warm up (`MaxConns=15`, `MinConns=2`).
3. Admin allowlists and default templates initialize asynchronously.
4. Collaboration coordinator establishes Redis channels for WebSocket broadcast.

Key environment toggles (defaults shown):

<Code
  code={`LAZY_INIT_ADMIN=true
ASYNC_TEMPLATE_SEED=true
SKIP_MIGRATION_CHECK=false
REDIS_POOL_SIZE=10`}
  language="env"
/>

Only disable the async flags when you need deterministic ordering for benchmarks.

## Crypto and Storage Hot Paths
- Encryption and decryption happen client-side. Server-side hot paths are Argon2id password verification and HMAC keyword search. Pin the backend CPU limit to at least one full core; Argon2id uses 64 MB memory and 3 iterations per authentication.
- Database queries for encrypted blobs are index-backed. Ensure `pg_stat_statements` stays less than 10% on `notes` reads; missing indexes indicate schema drift.
- If you observe high `leaflock_notes_total` write volume, scale PostgreSQL IOPS before tuning Go—crypto overhead is negligible compared to disk latency.

## WebSocket Performance
- The collaboration service multiplexes all rooms through Redis pub/sub. Keep latency < 50 ms between backend and Redis to avoid perceived typing lag.
- `leaflock_websocket_connections` should track close to the number of active editors. Sudden drops while `leaflock_active_users` remains high usually mean an upstream proxy closed idle sockets.
- When running behind NGINX or another proxy, increase idle timeouts to ≥ 120 s and forward the `Upgrade` headers unmodified.

## Batching and Caching
- HTTP handlers batch permission lookups in `workspace_service.go`. Monitor `leaflock_db_queries_total{operation="select"}`; if the rate spikes after a deploy, confirm the cache layer (`CACHE_WORKSPACE_PERMISSIONS=true`) remains enabled.
- Redis connection pool defaults handle 1k ops/sec. Raise `REDIS_POOL_SIZE` gradually and track `leaflock_redis_connections_active` to avoid exhausting file descriptors.

## Observability Checklist
- Enable structured logs with `LOG_LEVEL=info` (switch to `debug` during incident response only).
- Feed the Prometheus metrics described in `/admin/monitoring-and-backups` into alert rules for:
  - Startup latency (`time_to_ready` log field) > 30 seconds.
  - Error rate > 5% for `/api/v1/notes/*` endpoints.
  - Persistent Redis connection count near the pool limit.
- Use `docs/grafana-dashboard.json` as a baseline dashboard; add panels for Argon2 execution time if you patch the backend to expose it.

## Troubleshooting Slow Boots
1. **Long migration window**: ensure `SKIP_MIGRATION_CHECK` is `false` only in production, and run `make migrate` once before scaling.
2. **Redis handshake delays**: inspect `backend/logs` for `pubsub handshake took` warnings. Increase `REDIS_DIAL_TIMEOUT` if the platform injects latency.
3. **Proxy health probes**: point liveness probes to `/api/v1/health/live` and readiness probes to `/api/v1/health/ready`. Misconfigured probes will recycle pods before the async tasks complete.
4. **CPU throttling**: on Kubernetes set `resources.requests.cpu` to `500m` minimum; Argon2id will otherwise hit cgroup throttles.

LeafLock optimizations are safe-by-default; adjust only when metrics indicate a specific bottleneck.
